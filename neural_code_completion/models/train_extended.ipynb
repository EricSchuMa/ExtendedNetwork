{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import inspect\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import neural_code_completion.models.reader_pointer_extended as reader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from neural_code_completion.models.extendedNetwork import EN, ENInput, run_epoch\n",
        "from neural_code_completion.models.config import SmallConfig, TestConfig, BestConfig, ExperimentalConfig\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
        "outfile = './logs/output_extended_dev.txt'\n",
        "\n",
        "N_filename = '../pickle_data/PY_non_terminal_dev.pickle'\n",
        "T_filename = '../pickle_data/PY_terminal_1k_extended_dev.pickle'\n",
        "\n",
        "flags = tf.flags\n",
        "flags.DEFINE_string(\"logDir\", \"./logs/\" + str(datetime.date.today()) + \"/\", \"logging directory\")\n",
        "\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"model\", \"test\",\n",
        "    \"A type of model. Possible options are: small, medium, best.\")\n",
        "\n",
        "\n",
        "flags.DEFINE_bool(\"use_fp16\", False,\n",
        "                  \"Train using 16-bit floats instead of 32bit floats\")\n",
        "FLAGS = flags.FLAGS\n",
        "logging = tf.logging\n",
        "\n",
        "if FLAGS.model == \"test\":\n",
        "    outfile = './logs/TESToutput.txt'\n",
        "\n",
        "def get_config():\n",
        "    if FLAGS.model == \"small\":\n",
        "        return SmallConfig()\n",
        "    elif FLAGS.model == \"test\":\n",
        "        return TestConfig()\n",
        "    elif FLAGS.model == \"best\":\n",
        "        return BestConfig()\n",
        "    elif FLAGS.model == \"experimental\":\n",
        "        return ExperimentalConfig()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model: %s\", FLAGS.model)\n",
        "\n",
        "\n",
        "# This helper function taken from official TensorFlow documentation,\n",
        "# simply add some ops that take care of logging summaries\n",
        "def variable_summaries(var):\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    fout = open(outfile, 'a+')\n",
        "    print('\\n', time.asctime(time.localtime()), file=fout)\n",
        "    print('start a new experiment %s' % outfile, file=fout)\n",
        "    print('Using dataset %s and %s' % (N_filename, T_filename), file=fout)\n",
        "    print('condition on two, two layers', file=fout)\n",
        "\n",
        "    train_dataN, valid_dataN, vocab_sizeN, train_dataT, valid_dataT, vocab_sizeT, attn_size = reader.input_data(\n",
        "        N_filename, T_filename)\n",
        "\n",
        "    train_data = (train_dataN, train_dataT)\n",
        "    valid_data = (valid_dataN, valid_dataT)\n",
        "    vocab_size = (vocab_sizeN + 1, vocab_sizeT + 3)  # N is [w, eof], T is [w, unk, hog_id, eof]\n",
        "\n",
        "    config = get_config()\n",
        "    assert attn_size == config.attn_size  # make sure the attn_size used in generate terminal is the same as the configuration\n",
        "    config.vocab_size = vocab_size\n",
        "    eval_config = get_config()\n",
        "    eval_config.batch_size = config.batch_size * config.num_steps\n",
        "    eval_config.num_steps = 1\n",
        "    eval_config.vocab_size = vocab_size\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
        "\n",
        "        with tf.name_scope(\"Train\"):\n",
        "            train_input = ENInput(config=config, data=train_data, name=\"TrainInput\", FLAGS=FLAGS)\n",
        "            with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
        "                m = EN(is_training=True, config=config, input_=train_input, FLAGS=FLAGS)\n",
        "\n",
        "        with tf.name_scope(\"Valid\"):\n",
        "            valid_input = ENInput(config=config, data=valid_data, name=\"ValidInput\", FLAGS=FLAGS)\n",
        "            with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
        "                mvalid = EN(is_training=False, config=config, input_=valid_input, FLAGS=FLAGS)\n",
        "\n",
        "        print('total trainable variables', len(tf.trainable_variables()), '\\n\\n')\n",
        "        max_valid = 0\n",
        "        max_step = 0\n",
        "        saver = tf.train.Saver(tf.trainable_variables())\n",
        "\n",
        "        sv = tf.train.Supervisor(logdir=None, summary_op=None)\n",
        "        with sv.managed_session() as session:\n",
        "            train_dataT = session.run(m._input.input_dataT)\n",
        "            train_target = np.reshape(session.run(m._input.targetsT), [-1])\n",
        "            train_writer = tf.summary.FileWriter(FLAGS.logDir, graph=tf.get_default_graph())\n",
        "\n",
        "            for i in tqdm(range(config.max_max_epoch)):\n",
        "                lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
        "                m.assign_lr(session, config.learning_rate * lr_decay)\n",
        "                tqdm.write(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
        "\n",
        "                train_perplexity, train_accuracy = run_epoch(session, m, train_writer, i,\n",
        "                                                             eval_op=m.train_op, verbose=True)\n",
        "\n",
        "                tqdm.write(\n",
        "                    \"Epoch: %d Train Perplexity: %.3f Train Accuracy: %.3f\" % (i + 1, train_perplexity, train_accuracy))\n",
        "                print(\n",
        "                    \"Epoch: %d Train Perplexity: %.3f Train Accuracy: %.3f\" % (i + 1, train_perplexity, train_accuracy),\n",
        "                    file=fout)\n",
        "\n",
        "                \n",
        "                valid_perplexity, valid_accuracy = run_epoch(session, mvalid, train_writer, i)\n",
        "                tqdm.write(\"Epoch: %d Valid Perplexity: ~~%.3f Valid Accuracy: %.3f~\" % (\n",
        "                i + 1, valid_perplexity, valid_accuracy))\n",
        "                print(\"Epoch: %d Valid Perplexity: ~~%.3f Valid Accuracy: %.3f~\" % (\n",
        "                i + 1, valid_perplexity, valid_accuracy), file=fout)\n",
        "                if valid_accuracy > max_valid:\n",
        "                    max_valid = valid_accuracy\n",
        "                    max_step = i + 1\n",
        "\n",
        "\n",
        "                tqdm.write(\"Saving model to %s.\" % FLAGS.logDir)\n",
        "                saver.save(session, FLAGS.logDir + \"PMN-\", global_step=i)\n",
        "\n",
        "            tqdm.write('max step %d, max valid %.3f' % (max_step, max_valid))\n",
        "            tqdm.write('total time takes %.4f' % (time.time() - start_time))\n",
        "            print('max step %d, max valid %.3f' % (max_step, max_valid), file=fout)\n",
        "            print('total time takes %.3f' % (time.time() - start_time), file=fout)\n",
        "            fout.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}